{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Simple Topic Analysis Network Vis with Python, a Topic GUI (running Mallet), Excel, Gephi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Lynn Cherny (@arnicas) (March, 2015 and Oct, 2015) for PyLadies Boston and UMiami CCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling (sometimes known by the technical description LDA -- for \"latent dirichlet allocation\") is a statistical method for exploring the words in document collections and document relationships to one another via those words. In topic modeling, a topic is inferred from documents as a collection of likely words that are found in those documents. Some documents may be associated strongly with some topics and less strongly with others.   Here's an overview borrowed from this [presentation](http://www.slideshare.net/vitomirkovanovic/topic-modeling-for-learning-analytics-researchers-lak15-tutorial?utm_content=buffer89b5f&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer):\n",
    "\n",
    "<img src=\"images/goal_topic_modeling.png\">\n",
    "\n",
    "For instance, a news article about a Chinese soccer team might be loosely associated with a topic that includes words about Chinese politics, but more strongly associated with a topic that includes words about sports and soccer. Documents with similar vocabularies (content) will generally end up associated with similar topics, because the topics are constructed out of the observed frequencies of words in the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###A Few More Technical (and Non-Technical) References For Those Interested in LDA\n",
    "\n",
    "* [\"Probabilistic Topic Models\"](https://www.cs.princeton.edu/~blei/papers/Blei2012.pdf) by David Blei in CACM\n",
    "* [Dimensionality Reducation and Latent Topic Models](http://pages.cs.wisc.edu/~jerryzhu/cs769/latent.pdf) -  notes by Xiaojin Zhu, covers LSA and other methods, not just LDA\n",
    "* [\"Topic modeling made just simple enough\"](http://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/) by Ted Underwood from Digital Humanities perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But We're Mainly Trying to Visualize Results In This Demo\n",
    "\n",
    "\n",
    "There are a lot of more technical introductions to this topic, but this is meant to be a short talk illustrating the easiest way to get visual results without a lot of coding.\n",
    "\n",
    "We'll be using some chapters of well-known books from the [Gutenberg Project](https://www.gutenberg.org/) as well as some excerpts from modern best-sellers: \n",
    "\n",
    "* **Twilight (Stephanie Meyer)**, \n",
    "* **Fifty Shades of Gray (EL James)**, \n",
    "* and **The DaVinci Code** and **Angels and Demons (both by Dan Brown)**.  \n",
    "* **Jane Austen's Pride and Prejudice** and **Sense and Sensibility**, \n",
    "* **JM Barrie's Peter Pan**, \n",
    "* **Joseph Conrad's The Secret Agent**, \n",
    "* **AC Doyle's Sherlock Holmes** stories, \n",
    "* **George Eliot's Middlemarch**, \n",
    "* **Grimm's Fairytales**, \n",
    "* **CS Lewis's The Lion, the Witch, and the Wardrobe,** and \n",
    "* **RL Stevenson's Treasure Island.**\n",
    "\n",
    "I picked these since we'd expect to be able to see patterns in their content across both chapters and authors, but we might also learn something interesting, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the contents of the data folder we'll be using: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls data/mixed_chapters/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Alert: If you're on a Mac, check for a .DS_Store file here and delete it if you can before running the topic modeler.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic tool GUI we will use is available in this repo (and duplicated in my github repo, but it may not run for you without a fresh download, especially on Windows): https://code.google.com/p/topic-modeling-tool/.  This tool runs a GUI wrapper around [mallet](http://mallet.cs.umass.edu/), a state-of-the-art command line tool for topic modeling. After you've done this exercise, you can try running it without the GUI and exploring the other options in that package.\n",
    "\n",
    "Download it if you don't have it already.  The tool will be called **TopicModelingTool.jar.**\n",
    "\n",
    "Create a directory for output files, called **topic_output,** for example.  Doubleclick on the jar file to run it. Select the input directory **data/mixed_chapters** and the output directory you created. Click on **Advanced...** and change the settings to match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### It should look something like this:\n",
    "<img src=\"images/topicModelingTool_advanced.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it by clicking \"Learn Topics.\" Look at the 2 folders that have appeared in your ouput directory (e.g., topic_output), if this ran correctly:\n",
    "\n",
    "* output_csv\n",
    "* output_html\n",
    "\n",
    "If you want to browse around the output produced in the HTML folder, feel free (click on the _all_topics.html_ file to load it in a browser). **NB: You will probably see slightly different results... LDA training is non-deterministic and the element of randomness may lead to different results.**  But they will probably nevertheless be comparable; I've run several times on this data and gotten similar results each time.  For instance, with 12 topics, here is one set of results:\n",
    "\n",
    "<img src=\"images/topicModeling_allTopicsHTML.png\">\n",
    "\n",
    "Each topic is described by ten words associated with the documents in that topic. When you click on a topic, you can see some of the documents and the matching score for each:\n",
    "\n",
    "<img src=\"images/topicModeling_docsInTopic.png\">\n",
    "\n",
    "And if you click on a document, you can see how well it matches other topics in the set of topics. \n",
    "\n",
    "<img src=\"images/topicModeling_docMatches.png\">\n",
    "\n",
    "I find the HTML non-visual display kind of confusing. So we're going to make a network diagram instead. In this code, you want to set the path to your output directory for the csv files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import collections\n",
    "\n",
    "DIR = 'topic_output/output_csv/'\n",
    "topicWords = DIR + 'Topics_Words.csv'\n",
    "topicDocs = DIR + 'TopicsInDocs.csv'\n",
    "docsTopics = DIR + 'DocsInTopics.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will give you the 10 words per topic; we asked for 12 topics.\n",
    "# Be careful in your code - the topics are numbered starting with 1, not 0.\n",
    "\n",
    "def list_words_for_topics(filename):\n",
    "    \"\"\" Expects the Topics_Words csv file.\"\"\"\n",
    "    words = {}\n",
    "    with open(filename, 'rb') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for index, row in enumerate(reader):\n",
    "            if index > 0:  # skip first row\n",
    "                words[row[0]] = [x.strip() for x in row[1].split() if len(x.strip())>0]\n",
    "                print row\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_list = list_words_for_topics(topicWords)\n",
    "len(word_list)  # you will have as many as topics you requested - so, 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_list['4'] # listing the words in the topic number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, what are some things you notice about the words in the topics? Anything odd, or any patterns here?  Why?  Is it good, or bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Let's get the filenames mapped to the document id's which are used in most of these data files. The document filename and id is found in the DocsInTopics.csv.  While we're at it, let's get the authors, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_names_for_ids(filename):\n",
    "    \"\"\" Expects the DocsinTopics csv file.\"\"\"\n",
    "    \n",
    "    doc_titles = {}  # dictionary, the keys will be the doc id, the filename the value\n",
    "    doc_authors = {}\n",
    "    with open(filename, 'rb') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for index, row in enumerate(reader):\n",
    "            print row\n",
    "            if index > 0:  # skip first row making the data structure\n",
    "                title = row[3].split('/')[-1:][0]  # last element of the split\n",
    "                author = title.split('_')[0]\n",
    "                id = row[2]\n",
    "                print \"Parsed out \", title, author\n",
    "                doc_titles[id] = title\n",
    "                doc_authors[id] = author\n",
    "    return doc_titles, doc_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_ids, doc_authors = get_names_for_ids(docsTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_ids['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note here: If you see \".DS_Store\" as a document, you need to delete that file from the directory you input to the topic modeling tool GUI and rerun that.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One of the output files, the most useful for network drawing, is in an INSANE format where topic numbers alternate with scores for the percentage of the document matched to the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "<img src=\"images/TopicsInDocsCSV.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at it, we can see that some documents - like 3, 4, 5, 6 - don't have as many topics associated with them.  We used a cutoff value of .05 (5%) in the GUI when we ran the modeling, so we don't report any associations weaker than that. \n",
    "\n",
    "**But let's parse it nicely so we can use this in a visual.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This gets you the topic assigments and strength of association for each document and topic.\n",
    "\n",
    "def parse_topicsDocs(filename):\n",
    "    \"\"\" Filename input is the TopicsInDocs.csv file path. \"\"\"\n",
    "    \n",
    "    docs = {}\n",
    "    with open(filename, \"rb\") as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for index, row in enumerate(spamreader):\n",
    "            if index > 0:  # skip first row\n",
    "                print \"row\", row\n",
    "                docid = row[0]\n",
    "                topics = row[2:]\n",
    "                topics_dict = dict(zip(topics[::2],topics[1::2]))  #alternating\n",
    "                print docid, topics_dict\n",
    "                docs[docid] = topics_dict\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs_alltopics = parse_topicsDocs(topicDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This should match the first row of the spreadsheet!  Although it's not ordered the same way.\n",
    "\n",
    "docs_alltopics['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check you have the right number of documents in your new dictionary, to match files count (note if you have .DS_Store you will have a mismatch!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls data/mixed_chapters | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify the length of the dict is the number of docs in the data directory\n",
    "\n",
    "len(docs_alltopics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's do a simple first visualization of the data in Excel.\n",
    "\n",
    "\n",
    "To make a much simpler list of the data, I want to format it \"long form,\" with a document-topic pair per row.  The filename and author are just helpful for understanding the data, and for grouping by author if we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a simple csv format we could see in Excel: doc #, topic, strength, title\n",
    "\n",
    "def doc_topics_for_excel_table(docs_alltopics, doc_ids, doc_authors, filename):\n",
    "    ''' Produce a csv of docs, topics, scores, and filename.\n",
    "    \n",
    "    Args:\n",
    "        docs_alltopics: the output from read_doctopics\n",
    "        doc_ids: document ids to filename dict\n",
    "        filename: to save to\n",
    "    Output:\n",
    "        A csv file we can open in Excel.\n",
    "    '''\n",
    "    with open(filename, \"w\") as handle:\n",
    "        print \"DocID,Topic,Score,File,Author\"  # first row headers for cell below\n",
    "        \n",
    "        handle.write(\"DocID,Topic,Score,File,Author\\n\")  # the header of the file\n",
    "        for id, topics in docs_alltopics.iteritems():\n",
    "            #print x, docs_alltopics[x].keys()\n",
    "            for topic, score in topics.iteritems():\n",
    "                #print topic, score\n",
    "                print ','.join([str(id), \"Topic\"+str(topic), str(score), doc_ids[id], doc_authors[id]])\n",
    "                handle.write(','.join([str(id), \"Topic\"+str(topic), str(score), doc_ids[id], doc_authors[id] + \"\\n\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topics_for_excel_table(docs_alltopics, doc_ids, doc_authors, 'data/for_excel.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now open that file in Excel and do some nice analysis / viewing of the topics in a pivot table!\n",
    "\n",
    "####  In a pivot table, you can see the matrix of files and topics and scores, and sort as you like. Note that this table view will have different numbers than yours...\n",
    "<img src=\"images/Excel_Pivot.png\" width=\"90%\">\n",
    "\n",
    "Some things are clear from this view: the Austen files are all associated strongly with the same topic numbers (7 and  11 in this picture), and not with most other topics. Topics 11 and 9 are pretty diffuse - weakly matching most of the documents.  The strength of author to topic, even across books, is remarkable for some authors, like Dan Brown, Doyle, and EL James.  But we should expect that, knowing what we know about their writing!\n",
    "\n",
    "Let's try a network view now.  Let's generate that data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Views\n",
    "\n",
    "One way to visualize this data is in a network - since we have relationships among a bunch of objects. \n",
    "A simple first pass is to imagine 2 node types: documents, and topics.  The links between these nodes \n",
    "are the relations of the documents to topics.  We can also scale the edge lines by the strength of the relationship to the topic.\n",
    "\n",
    "<img src=\"images/simple_network.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [Gephi network visualization tool](http://gephi.org), we can import simple nodes (the dots above) and edges, the links between them, from CSV files.  So we have to do some data munging to write those out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_nodes_file(doc_ids, doc_authors, topic_list=None, filename='nodes.csv'):\n",
    "    ''' Produce a csv of docs, topics, scores, and filename.\n",
    "    \n",
    "    Args:\n",
    "        doc_ids: dict with document id keys to filename values\n",
    "        doc_authors: dict with doc id keys to authors of docs\n",
    "        topic_list (opt'l): a dict of the words for each topic, the output from list_words_for_topics\n",
    "        filename: file to to save to, default nodes.csv\n",
    "    Output:\n",
    "        A dict of node id numbers for docs and topics, keys being 'Doc' + id or 'Topic' + id.\n",
    "        This is because Gephi needs unique node id's.\n",
    "    '''\n",
    "    \n",
    "    idNumbering = {}\n",
    "    \n",
    "    counter = 1\n",
    "    for doc in doc_ids.keys():\n",
    "        idNumbering['Doc' + doc] = counter\n",
    "        counter += 1\n",
    "    \n",
    "    if topic_list:\n",
    "        for topic in word_list:\n",
    "            idNumbering['Topic' + topic] = counter\n",
    "            counter += 1\n",
    "                \n",
    "    with open(filename, \"w\") as handle:\n",
    "        print \"Id,Label,Type,Author\"  # first row headers for cell below\n",
    "        handle.write(\"Id,Label,Type,Author\\n\")  # the header of the file\n",
    "        for doc in doc_ids.keys():\n",
    "            string = ','.join([str(idNumbering['Doc' + doc]),\n",
    "                        doc_ids[doc].replace('.txt',''), 'Doc',\n",
    "                        str(doc_authors[doc])])\n",
    "            print string\n",
    "            handle.write(string + '\\n')\n",
    "        if topic_list:\n",
    "            for topic in word_list.keys():\n",
    "                string = ','.join([str(idNumbering['Topic' + topic]),\n",
    "                            ':'.join(['Topic' + topic] + word_list[topic][0:4]), 'Topic',\n",
    "                            'None'])\n",
    "                print string\n",
    "                handle.write(string + '\\n')\n",
    "    \n",
    "    return idNumbering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numbering = make_nodes_file(doc_ids, doc_authors, topic_list=word_list, filename = 'data/nodes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_edge_file(doctopics, filename = 'edges.csv'):\n",
    "    \"\"\" \n",
    "    Print out the edges, the links from documents to topics, with scores, as csv.\n",
    "    Args:\n",
    "        doctopics: the result of parse_topicsDocs\n",
    "        filename: file to save result to as csv\n",
    "    Output:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'w') as handle:\n",
    "        handle.write(\"Source,Target,Weight\\n\")\n",
    "        print \"Source,Target,Weight\"\n",
    "        for doc, topics in doctopics.iteritems():\n",
    "            for topic, score in topics.iteritems():\n",
    "                string = ','.join([str(numbering['Doc' + str(doc)]), \n",
    "                                   str(numbering[\"Topic\"+str(topic)]), \n",
    "                                   str(score)])\n",
    "                print string\n",
    "                handle.write(string + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_edge_file(docs_alltopics, filename = 'data/edges.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now open Gephi.\n",
    "\n",
    "Load the nodes and edges files into Gephi.  Get the Data Table window open. \n",
    "\n",
    "<img src='images/Gephi_ImportSpreadsheet.png'>\n",
    "\n",
    "Load the nodes file and the edges file using the \"import spreadsheet\" button.\n",
    "\n",
    "<img src='images/Gephi_ImportNodes.png'>\n",
    "\n",
    "Don't forget the edges file too!\n",
    "\n",
    "<img src='images/Gephi_ImportEdges.png'>\n",
    "\n",
    "If you need more help, try this [Gephi help-page](https://github.com/gephi/gephi/wiki/Import-CSV-Data).\n",
    "\n",
    "Now switch to the Overview tab to lay out your nodes and size/color things.  Use the Preview tab to make a pretty version to print. There are some helpful instructions at the beginning of the [GephiToSigmaJS pdf](files/GephiToSigmaJS_Grimms.pdf) in the files directory, but the end of it isn't relevant here.\n",
    "\n",
    "Some things to try:\n",
    "* Color (\"partition\") nodes by type - topic or document.\n",
    "* Color by author - so you can see documents by the same author, regardless of the chapter/title.\n",
    "* Size nodes by degree - the number of edges attached to them.  This shows you the more \"popular\" topics.\n",
    "* Use a force (force atlas 2) layout, so that items more connected are closer together.\n",
    "* Use \"label adjust\" to shift nodes a little, to prevent overlap of label and node. You'll still need to hand adjust for a good final version.\n",
    "* Adjust in Overview, and fine-tune the visual for export in Preview.\n",
    "\n",
    "\n",
    "When you're done, your Preview might look something like this, where thicker edges mean stronger relationship:\n",
    "\n",
    "<img src='images/Docs_Topics_Gephi.png' width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image, you can see the Austen files grouped together closely to a topic with \"elinor\" in it.  The EL James files are close to their topic node.  Thicker lines represent stronger ties.  You can see some strong ties crossing the entire network.\n",
    "\n",
    "## Suppose we want a network of just the documents, removing the topics in between?  \n",
    "\n",
    "We can do a little data munging to link documents that belong to a topic, based on a scaled average of the weights.  First, let's use the nicely formatted excel file to get a dictionary of the topics and their documents and weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "topics = defaultdict(list)\n",
    "\n",
    "with open('data/for_excel.csv', 'r') as handle:\n",
    "    reader = csv.reader(handle)\n",
    "    next(reader)  # skip the first row which is labels\n",
    "    for row in reader:\n",
    "        doc = row[0]\n",
    "        topic = row[1]\n",
    "        weight = row[2]\n",
    "        topics[topic].append((doc, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a dict of topics with a list of doc-weight pairs per topic.\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics['Topic2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a little clever data munging to make relationships between all the documents that are associated with a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A short way to make a set of pairs of items in a list\n",
    "import itertools\n",
    "pairs = [24,34,454,54]\n",
    "[x for x in itertools.combinations(pairs,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can apply this principle to the pairs of the doc and score, too:\n",
    "\n",
    "[x for x in itertools.combinations(topics['Topic2'],2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's create the edges file without the topics in the mix...  We'll also do a little dirty math to try to adjust the scores to make a variant that relates two documents to each other, based on their original relationship to the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "edgesScores = defaultdict(list)\n",
    "\n",
    "for topic, doclist in topics.iteritems():\n",
    "    # make the pairs from all the document, weight items.  First filter at whatever weight you want.\n",
    "    filtered = [x for x in doclist if float(x[1]) >= .30]\n",
    "    \n",
    "    # filtering because there are just TOO MANY LINKS otherwise.  Trust me, I tried it first without.\n",
    "    print \"Topic\", topic, \"originally\", len(doclist), \"filtered to\", len(filtered)\n",
    "    combos = [x for x in itertools.combinations(filtered,2)]\n",
    "    for pair in combos:\n",
    "        #print pair\n",
    "        node1 = pair[0][0]\n",
    "        node2 = pair[1][0]\n",
    "        weight1 = float(pair[0][1])\n",
    "        weight2 = float(pair[1][1])\n",
    "        if weight1 and weight2:\n",
    "            # an approximation of the distance based on similarity in this topic\n",
    "            if weight1 != weight2:\n",
    "                weight = (1 / abs(weight1 - weight2)) / 10\n",
    "            else:  # just make it high if the scores are the same\n",
    "                weight = 100\n",
    "            if node2 < node1:\n",
    "                # swap to keep the same ordering everywhere\n",
    "                node1, node2 = node2, node1\n",
    "        edgesScores[(node1,node2)].append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(edgesScores.iteritems())[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('data/edges2.csv', 'w') as handle:\n",
    "    print 'Source,Target,Weight'\n",
    "    handle.write('Source,Target,Weight\\n')\n",
    "    for pair, weights in edgesScores.iteritems():\n",
    "        source = str(numbering['Doc' + pair[0]])\n",
    "        target = str(numbering['Doc' + pair[1]])\n",
    "        weight = round(sum(weights)/len(weights),2)  # avg, rounded to 2 decimal places\n",
    "        print ','.join([source, target, str(weight)])\n",
    "        handle.write(','.join([source, target, str(weight)]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write out a new nodes files without the topics in there.  That's why it was optional!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numbering = make_nodes_file(doc_ids, doc_authors, filename = 'data/nodes2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The view in Gephi differs now...\n",
    "\n",
    "After loading that into Gephi and doing some work on it, you get something like this... where, as expected, most of the chapters by the same author are linked.  There are a few interesting oddities, though!\n",
    "<img src=\"images/Docs_Only.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Export a Web Graph\n",
    "\n",
    "If you want to export an interactive web page for your graph, you can use the Gephi Plugin for sigma.js. There are instructions in the file [GephiToSigmaJS_Grimms.pdf](https://github.com/arnicas/TopicsNetworksPyladies/blob/master/files/GephiToSigmaJS_Grimms.pdf).\n",
    "\n",
    "Before you run the server to display your sigma.js graph, you should make some modifications to the display settings in the config file.  They are described in the PDF.  Or, you can run your server using files/run_network.py and it will modify the file for you:\n",
    "\n",
    "````\n",
    ">python run_network.py [network_dir] [optl port]\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### More Reading/Code\n",
    "\n",
    "For some more fun, you can run topic modeling in Python directly using Gensim (or other packages like [lda](http://pythonhosted.org/lda/)).\n",
    "\n",
    "* A recent example of topic modeling with gensim on Shakespeare's sonnets: http://nbviewer.ipython.org/github/sgsinclair/alta/blob/master/ipynb/TopicModelling.ipynb\n",
    "* A tutorial slidedeck focused mostly on R, not Python: http://www.slideshare.net/vitomirkovanovic/topic-modeling-for-learning-analytics-researchers-lak15-tutorial?utm_content=buffer89b5f&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer\n",
    "* The gensim package's intro material on LDA: http://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation\n",
    "* Slightly more stuff in my longer tutorial on github: [TopicsPyhonGephi](http://github.com/arnicas/TopicsPythonGephi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
